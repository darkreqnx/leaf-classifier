{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_uHdN2DS8nM5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N_w02yxFINzZ"
   },
   "outputs": [],
   "source": [
    "data_url = \"https://raw.githubusercontent.com/darkreqnx/leaf-classifier/master/img_dataset.csv?token=AIOJSRNXUP63AVJTN7BI67267RU5I\"\n",
    "df = pd.read_csv(data_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "4_hZxYf_8rPJ",
    "outputId": "5112813b-87e3-4e47-de11-a92350fc4fbb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>smoothness0</th>\n",
       "      <th>leaflet_desc0</th>\n",
       "      <th>leaflet_desc1</th>\n",
       "      <th>leaflet_desc2</th>\n",
       "      <th>leaflet_desc3</th>\n",
       "      <th>leaflet_desc4</th>\n",
       "      <th>leaflet_desc5</th>\n",
       "      <th>leaflet_desc6</th>\n",
       "      <th>gabor0</th>\n",
       "      <th>gabor1</th>\n",
       "      <th>gabor2</th>\n",
       "      <th>gabor3</th>\n",
       "      <th>gabor4</th>\n",
       "      <th>gabor5</th>\n",
       "      <th>gabor6</th>\n",
       "      <th>gabor7</th>\n",
       "      <th>color0</th>\n",
       "      <th>color1</th>\n",
       "      <th>color2</th>\n",
       "      <th>color3</th>\n",
       "      <th>color4</th>\n",
       "      <th>color5</th>\n",
       "      <th>color6</th>\n",
       "      <th>color7</th>\n",
       "      <th>color8</th>\n",
       "      <th>color9</th>\n",
       "      <th>color10</th>\n",
       "      <th>color11</th>\n",
       "      <th>fft0</th>\n",
       "      <th>fft1</th>\n",
       "      <th>fft2</th>\n",
       "      <th>fft3</th>\n",
       "      <th>fft4</th>\n",
       "      <th>fft5</th>\n",
       "      <th>fft6</th>\n",
       "      <th>fft7</th>\n",
       "      <th>fft8</th>\n",
       "      <th>fft9</th>\n",
       "      <th>fft10</th>\n",
       "      <th>...</th>\n",
       "      <th>geom11</th>\n",
       "      <th>geom12</th>\n",
       "      <th>geom13</th>\n",
       "      <th>geom14</th>\n",
       "      <th>geom15</th>\n",
       "      <th>geom16</th>\n",
       "      <th>geom17</th>\n",
       "      <th>geom18</th>\n",
       "      <th>geom19</th>\n",
       "      <th>geom20</th>\n",
       "      <th>geom21</th>\n",
       "      <th>geom22</th>\n",
       "      <th>geom23</th>\n",
       "      <th>geom24</th>\n",
       "      <th>geom25</th>\n",
       "      <th>geom26</th>\n",
       "      <th>geom27</th>\n",
       "      <th>geom28</th>\n",
       "      <th>geom29</th>\n",
       "      <th>geom30</th>\n",
       "      <th>geom31</th>\n",
       "      <th>geom32</th>\n",
       "      <th>geom33</th>\n",
       "      <th>geom34</th>\n",
       "      <th>geom35</th>\n",
       "      <th>geom36</th>\n",
       "      <th>geom37</th>\n",
       "      <th>geom38</th>\n",
       "      <th>geom39</th>\n",
       "      <th>geom40</th>\n",
       "      <th>geom41</th>\n",
       "      <th>geom42</th>\n",
       "      <th>geom43</th>\n",
       "      <th>geom44</th>\n",
       "      <th>geom45</th>\n",
       "      <th>geom46</th>\n",
       "      <th>geom47</th>\n",
       "      <th>geom48</th>\n",
       "      <th>geom49</th>\n",
       "      <th>geom50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.990136</td>\n",
       "      <td>0.293172</td>\n",
       "      <td>5.898140</td>\n",
       "      <td>57.140699</td>\n",
       "      <td>38.832976</td>\n",
       "      <td>239.002092</td>\n",
       "      <td>182.043951</td>\n",
       "      <td>2736.354694</td>\n",
       "      <td>0.767506</td>\n",
       "      <td>0.803687</td>\n",
       "      <td>0.760917</td>\n",
       "      <td>0.733909</td>\n",
       "      <td>0.755205</td>\n",
       "      <td>0.782215</td>\n",
       "      <td>0.795676</td>\n",
       "      <td>0.751683</td>\n",
       "      <td>0.015092</td>\n",
       "      <td>0.005299</td>\n",
       "      <td>0.844378</td>\n",
       "      <td>0.133894</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>0.000211</td>\n",
       "      <td>0.000815</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.070427</td>\n",
       "      <td>0.068695</td>\n",
       "      <td>0.060660</td>\n",
       "      <td>0.049750</td>\n",
       "      <td>0.137163</td>\n",
       "      <td>0.117566</td>\n",
       "      <td>0.008226</td>\n",
       "      <td>0.037063</td>\n",
       "      <td>0.054877</td>\n",
       "      <td>0.036925</td>\n",
       "      <td>0.037894</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.071620</td>\n",
       "      <td>-0.752859</td>\n",
       "      <td>0.611270</td>\n",
       "      <td>-1.097490</td>\n",
       "      <td>0.551494</td>\n",
       "      <td>0.196806</td>\n",
       "      <td>0.154064</td>\n",
       "      <td>0.018376</td>\n",
       "      <td>0.384700</td>\n",
       "      <td>0.095523</td>\n",
       "      <td>0.985284</td>\n",
       "      <td>1.666760</td>\n",
       "      <td>0.908266</td>\n",
       "      <td>1.055867</td>\n",
       "      <td>-1.011291</td>\n",
       "      <td>-0.457053</td>\n",
       "      <td>0.770790</td>\n",
       "      <td>-0.277475</td>\n",
       "      <td>-0.682188</td>\n",
       "      <td>0.551631</td>\n",
       "      <td>-0.746709</td>\n",
       "      <td>-0.328224</td>\n",
       "      <td>0.357519</td>\n",
       "      <td>-0.364019</td>\n",
       "      <td>-0.518048</td>\n",
       "      <td>-0.023039</td>\n",
       "      <td>-0.797469</td>\n",
       "      <td>0.996792</td>\n",
       "      <td>1.399421</td>\n",
       "      <td>-0.012732</td>\n",
       "      <td>0.267327</td>\n",
       "      <td>-1.018589</td>\n",
       "      <td>-0.007033</td>\n",
       "      <td>0.138642</td>\n",
       "      <td>0.389550</td>\n",
       "      <td>0.097027</td>\n",
       "      <td>0.441639</td>\n",
       "      <td>1.124257</td>\n",
       "      <td>0.695187</td>\n",
       "      <td>0.624561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.947406</td>\n",
       "      <td>0.337538</td>\n",
       "      <td>6.429441</td>\n",
       "      <td>53.765871</td>\n",
       "      <td>50.358713</td>\n",
       "      <td>207.696895</td>\n",
       "      <td>151.489274</td>\n",
       "      <td>1581.968056</td>\n",
       "      <td>0.941950</td>\n",
       "      <td>0.847207</td>\n",
       "      <td>0.815962</td>\n",
       "      <td>0.808848</td>\n",
       "      <td>0.779815</td>\n",
       "      <td>0.818916</td>\n",
       "      <td>0.846362</td>\n",
       "      <td>0.917206</td>\n",
       "      <td>0.018860</td>\n",
       "      <td>0.012823</td>\n",
       "      <td>0.452758</td>\n",
       "      <td>0.515107</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>0</td>\n",
       "      <td>0.081972</td>\n",
       "      <td>0.083511</td>\n",
       "      <td>0.072986</td>\n",
       "      <td>0.054633</td>\n",
       "      <td>0.083010</td>\n",
       "      <td>0.127735</td>\n",
       "      <td>0.045627</td>\n",
       "      <td>0.010247</td>\n",
       "      <td>0.040070</td>\n",
       "      <td>0.048555</td>\n",
       "      <td>0.046960</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.898505</td>\n",
       "      <td>-0.421081</td>\n",
       "      <td>0.889103</td>\n",
       "      <td>-0.878074</td>\n",
       "      <td>0.980123</td>\n",
       "      <td>-0.873983</td>\n",
       "      <td>0.199895</td>\n",
       "      <td>0.110950</td>\n",
       "      <td>0.600093</td>\n",
       "      <td>-0.317701</td>\n",
       "      <td>-0.085417</td>\n",
       "      <td>1.302360</td>\n",
       "      <td>0.220882</td>\n",
       "      <td>0.155951</td>\n",
       "      <td>-1.067050</td>\n",
       "      <td>-0.486090</td>\n",
       "      <td>0.724310</td>\n",
       "      <td>0.610906</td>\n",
       "      <td>-0.156947</td>\n",
       "      <td>-0.667784</td>\n",
       "      <td>-0.189414</td>\n",
       "      <td>-0.328224</td>\n",
       "      <td>-0.551312</td>\n",
       "      <td>0.301024</td>\n",
       "      <td>0.162199</td>\n",
       "      <td>-0.466444</td>\n",
       "      <td>-0.562133</td>\n",
       "      <td>-0.672715</td>\n",
       "      <td>-0.855704</td>\n",
       "      <td>-0.638822</td>\n",
       "      <td>1.355466</td>\n",
       "      <td>-0.573917</td>\n",
       "      <td>-0.396909</td>\n",
       "      <td>-0.366315</td>\n",
       "      <td>0.564329</td>\n",
       "      <td>1.121807</td>\n",
       "      <td>1.548557</td>\n",
       "      <td>1.111618</td>\n",
       "      <td>0.726782</td>\n",
       "      <td>0.565695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.960641</td>\n",
       "      <td>0.266033</td>\n",
       "      <td>7.058012</td>\n",
       "      <td>62.578195</td>\n",
       "      <td>53.338541</td>\n",
       "      <td>242.886805</td>\n",
       "      <td>179.513231</td>\n",
       "      <td>2376.196104</td>\n",
       "      <td>0.898163</td>\n",
       "      <td>0.830881</td>\n",
       "      <td>0.805372</td>\n",
       "      <td>0.770342</td>\n",
       "      <td>0.755695</td>\n",
       "      <td>0.769563</td>\n",
       "      <td>0.822595</td>\n",
       "      <td>0.918369</td>\n",
       "      <td>0.014501</td>\n",
       "      <td>0.004747</td>\n",
       "      <td>0.550366</td>\n",
       "      <td>0.430199</td>\n",
       "      <td>0.000166</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0</td>\n",
       "      <td>0.067323</td>\n",
       "      <td>0.060668</td>\n",
       "      <td>0.053945</td>\n",
       "      <td>0.035244</td>\n",
       "      <td>0.105082</td>\n",
       "      <td>0.127844</td>\n",
       "      <td>0.030847</td>\n",
       "      <td>0.033383</td>\n",
       "      <td>0.066364</td>\n",
       "      <td>0.046371</td>\n",
       "      <td>0.020601</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.047215</td>\n",
       "      <td>-0.670810</td>\n",
       "      <td>0.572735</td>\n",
       "      <td>-0.816224</td>\n",
       "      <td>0.704392</td>\n",
       "      <td>0.078168</td>\n",
       "      <td>-0.196897</td>\n",
       "      <td>0.056299</td>\n",
       "      <td>0.243914</td>\n",
       "      <td>0.248556</td>\n",
       "      <td>1.244132</td>\n",
       "      <td>1.569838</td>\n",
       "      <td>1.067666</td>\n",
       "      <td>1.281215</td>\n",
       "      <td>-1.047111</td>\n",
       "      <td>-0.515518</td>\n",
       "      <td>0.550105</td>\n",
       "      <td>0.045615</td>\n",
       "      <td>-0.687638</td>\n",
       "      <td>0.224539</td>\n",
       "      <td>-0.746709</td>\n",
       "      <td>-0.328224</td>\n",
       "      <td>-0.551312</td>\n",
       "      <td>-0.644368</td>\n",
       "      <td>-0.240554</td>\n",
       "      <td>0.040023</td>\n",
       "      <td>-0.886653</td>\n",
       "      <td>-0.734647</td>\n",
       "      <td>-0.712324</td>\n",
       "      <td>-0.526449</td>\n",
       "      <td>0.841678</td>\n",
       "      <td>-0.103730</td>\n",
       "      <td>-0.243852</td>\n",
       "      <td>-0.452981</td>\n",
       "      <td>0.032249</td>\n",
       "      <td>0.043864</td>\n",
       "      <td>0.544354</td>\n",
       "      <td>1.198798</td>\n",
       "      <td>0.831943</td>\n",
       "      <td>-0.459434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.956420</td>\n",
       "      <td>0.227015</td>\n",
       "      <td>4.864612</td>\n",
       "      <td>48.330907</td>\n",
       "      <td>44.045431</td>\n",
       "      <td>216.004630</td>\n",
       "      <td>163.878003</td>\n",
       "      <td>1510.848786</td>\n",
       "      <td>0.797747</td>\n",
       "      <td>0.774420</td>\n",
       "      <td>0.658974</td>\n",
       "      <td>0.650082</td>\n",
       "      <td>0.651573</td>\n",
       "      <td>0.650955</td>\n",
       "      <td>0.668357</td>\n",
       "      <td>0.715309</td>\n",
       "      <td>0.019888</td>\n",
       "      <td>0.004747</td>\n",
       "      <td>0.575858</td>\n",
       "      <td>0.399218</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>0</td>\n",
       "      <td>0.093941</td>\n",
       "      <td>0.069459</td>\n",
       "      <td>0.080111</td>\n",
       "      <td>0.020528</td>\n",
       "      <td>0.116983</td>\n",
       "      <td>0.105256</td>\n",
       "      <td>0.050896</td>\n",
       "      <td>0.045107</td>\n",
       "      <td>0.043655</td>\n",
       "      <td>0.060127</td>\n",
       "      <td>0.031633</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.017493</td>\n",
       "      <td>-0.604512</td>\n",
       "      <td>0.830339</td>\n",
       "      <td>-0.953665</td>\n",
       "      <td>1.416105</td>\n",
       "      <td>-0.409719</td>\n",
       "      <td>0.117211</td>\n",
       "      <td>-0.023152</td>\n",
       "      <td>-0.224811</td>\n",
       "      <td>0.568606</td>\n",
       "      <td>0.984145</td>\n",
       "      <td>0.804790</td>\n",
       "      <td>0.607204</td>\n",
       "      <td>-0.341478</td>\n",
       "      <td>-1.037871</td>\n",
       "      <td>-0.475758</td>\n",
       "      <td>0.748974</td>\n",
       "      <td>0.062967</td>\n",
       "      <td>-0.388759</td>\n",
       "      <td>-0.667784</td>\n",
       "      <td>-0.169367</td>\n",
       "      <td>-0.328224</td>\n",
       "      <td>-0.026609</td>\n",
       "      <td>0.020068</td>\n",
       "      <td>0.035388</td>\n",
       "      <td>-0.311361</td>\n",
       "      <td>-0.492448</td>\n",
       "      <td>-0.415298</td>\n",
       "      <td>-0.511806</td>\n",
       "      <td>-0.529879</td>\n",
       "      <td>0.944517</td>\n",
       "      <td>-0.553011</td>\n",
       "      <td>-0.119947</td>\n",
       "      <td>-0.107208</td>\n",
       "      <td>0.073492</td>\n",
       "      <td>0.719025</td>\n",
       "      <td>1.621169</td>\n",
       "      <td>1.107195</td>\n",
       "      <td>0.751325</td>\n",
       "      <td>0.751414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.945802</td>\n",
       "      <td>0.229219</td>\n",
       "      <td>5.487981</td>\n",
       "      <td>49.724115</td>\n",
       "      <td>42.011903</td>\n",
       "      <td>179.446371</td>\n",
       "      <td>134.766465</td>\n",
       "      <td>1346.737691</td>\n",
       "      <td>1.057724</td>\n",
       "      <td>0.933728</td>\n",
       "      <td>0.704337</td>\n",
       "      <td>0.651289</td>\n",
       "      <td>0.659790</td>\n",
       "      <td>0.686938</td>\n",
       "      <td>0.714581</td>\n",
       "      <td>0.865413</td>\n",
       "      <td>0.025128</td>\n",
       "      <td>0.008429</td>\n",
       "      <td>0.363056</td>\n",
       "      <td>0.603132</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>0</td>\n",
       "      <td>0.089514</td>\n",
       "      <td>0.096912</td>\n",
       "      <td>0.073890</td>\n",
       "      <td>0.043355</td>\n",
       "      <td>0.044485</td>\n",
       "      <td>0.107758</td>\n",
       "      <td>0.090597</td>\n",
       "      <td>0.062963</td>\n",
       "      <td>0.019032</td>\n",
       "      <td>0.027247</td>\n",
       "      <td>0.058747</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.929655</td>\n",
       "      <td>-0.293354</td>\n",
       "      <td>1.549450</td>\n",
       "      <td>-0.998710</td>\n",
       "      <td>2.369302</td>\n",
       "      <td>-1.318394</td>\n",
       "      <td>1.607098</td>\n",
       "      <td>0.153506</td>\n",
       "      <td>0.399876</td>\n",
       "      <td>-0.012222</td>\n",
       "      <td>1.111127</td>\n",
       "      <td>0.331963</td>\n",
       "      <td>-0.218579</td>\n",
       "      <td>-1.534208</td>\n",
       "      <td>-1.050212</td>\n",
       "      <td>-0.568134</td>\n",
       "      <td>0.491667</td>\n",
       "      <td>0.342536</td>\n",
       "      <td>-0.361314</td>\n",
       "      <td>-0.297110</td>\n",
       "      <td>-0.746709</td>\n",
       "      <td>-0.328224</td>\n",
       "      <td>-0.551312</td>\n",
       "      <td>0.062469</td>\n",
       "      <td>0.096484</td>\n",
       "      <td>-0.226575</td>\n",
       "      <td>-0.509470</td>\n",
       "      <td>-0.750551</td>\n",
       "      <td>-0.445796</td>\n",
       "      <td>-0.460051</td>\n",
       "      <td>1.586457</td>\n",
       "      <td>-0.934877</td>\n",
       "      <td>-0.531001</td>\n",
       "      <td>0.913120</td>\n",
       "      <td>1.788415</td>\n",
       "      <td>2.650751</td>\n",
       "      <td>1.517426</td>\n",
       "      <td>0.970579</td>\n",
       "      <td>0.514335</td>\n",
       "      <td>-0.506358</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 130 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   code  smoothness0  leaflet_desc0  ...    geom48    geom49    geom50\n",
       "0     0     0.990136       0.293172  ...  1.124257  0.695187  0.624561\n",
       "1     0     0.947406       0.337538  ...  1.111618  0.726782  0.565695\n",
       "2     0     0.960641       0.266033  ...  1.198798  0.831943 -0.459434\n",
       "3     0     0.956420       0.227015  ...  1.107195  0.751325  0.751414\n",
       "4     0     0.945802       0.229219  ...  0.970579  0.514335 -0.506358\n",
       "\n",
       "[5 rows x 130 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "de-tpupJ7dTV"
   },
   "outputs": [],
   "source": [
    "def util_vanilla(base_dataset):\n",
    "  # stratified train-test split\n",
    "  # exluding single valued class\n",
    "  idx_single_val = base_dataset[base_dataset['code'] == 35].index[0]\n",
    "  trial_dataset = base_dataset.drop([idx_single_val])\n",
    "  # stratified train-test split\n",
    "  train, test = train_test_split(trial_dataset, test_size=0.3, stratify=trial_dataset['code'], random_state=0)\n",
    "  # reintroduce excluded value in train, split into X,y\n",
    "  train = train.append(base_dataset.iloc[idx_single_val])\n",
    "  # random train-test split\n",
    "  # train, test = train_test_split(trial_dataset, test_size=0.3, random_state=0)\n",
    "  X_train = train.drop(['code'], axis=1)\n",
    "  y_train = train['code']\n",
    "  X_test = test.drop(['code'], axis=1)\n",
    "  y_test = test['code']\n",
    "  return X_train, y_train, X_test, y_test\n",
    "\n",
    "def vanilla_Dt_classf(base_dataset):\n",
    "  X_train, y_train, X_test, y_test = util_vanilla(base_dataset)\n",
    "  # decision tree classf\n",
    "  tree_clf = tree.DecisionTreeClassifier(criterion=\"entropy\", random_state=23)\n",
    "  tree_clf = tree_clf.fit(X_train, y_train)\n",
    "  y_predict_tree = tree_clf.predict(X_test)\n",
    "  # check score\n",
    "  print(\"decision tree classifier:\", accuracy_score(y_test, y_predict_tree))\n",
    "  return tree_clf\n",
    "\n",
    "def vanilla_Rf_classf(base_dataset):\n",
    "  X_train, y_train, X_test, y_test = util_vanilla(base_dataset)\n",
    "  # random forest classf\n",
    "  rf_clf = RandomForestClassifier(criterion=\"entropy\", random_state=23, n_jobs=-1)\n",
    "  rf_clf.fit(X_train, y_train)\n",
    "  y_predict_rf = rf_clf.predict(X_test)\n",
    "  # check score  \n",
    "  print(\"random forest classifier:\", accuracy_score(y_test, y_predict_rf))\n",
    "  return rf_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "_X7QdRcC3g_6",
    "outputId": "93acbd1d-6e70-49a8-f2c0-619f0cd3a3b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decision tree classifier: 0.6985151710781149\n",
      "random forest classifier: 0.921885087153002\n"
     ]
    }
   ],
   "source": [
    "# run the classifiers out of the box\n",
    "\n",
    "tree_clf = vanilla_Dt_classf(df)\n",
    "rf_clf = vanilla_Rf_classf(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G0fi--wcppLT"
   },
   "source": [
    "# feature selection: using Dt and Rf feat. importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "id": "nzqxOAkY8oKu",
    "outputId": "d7e57f83-c545-4cc2-d95f-aa1897792b16"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# feature importance - decision tree\n",
    "\n",
    "importance = tree_clf.feature_importances_\n",
    "# summarize feature importance\n",
    "# for i,v in enumerate(importance):\n",
    "\t# print('Feature: %0d, Score: %.5f' % (i,v))\n",
    "# plot feature importance\n",
    "plt.bar([x for x in range(len(importance))], importance)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 344
    },
    "colab_type": "code",
    "id": "2fidgm0wQJGw",
    "outputId": "c9ed5cf2-046f-48f7-9958-96763495815e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shortlisting top 75.0 % features...\n",
      "decision tree classifier: 0.7075532601678503\n",
      "..............\n",
      "shortlisting top 80.0 % features...\n",
      "decision tree classifier: 0.7043253712072305\n",
      "..............\n",
      "shortlisting top 85.0 % features...\n",
      "decision tree classifier: 0.7049709489993544\n",
      "..............\n",
      "shortlisting top 90.0 % features...\n",
      "decision tree classifier: 0.7204648160103292\n",
      "..............\n",
      "shortlisting top 95.0 % features...\n",
      "decision tree classifier: 0.7017430600387347\n",
      "..............\n",
      "shortlisting top 100.0 % features...\n",
      "decision tree classifier: 0.6959328599096191\n",
      "..............\n"
     ]
    }
   ],
   "source": [
    "# shortlist x% cumulative importance\n",
    "\n",
    "tree_feature_importance = pd.DataFrame({'tree_feature':list(df.columns[1:]),'importance':list(importance)})\n",
    "tree_feature_importance = tree_feature_importance.sort_values('importance', ascending=False).reset_index()\n",
    "tree_feature_importance['cumulative'] = tree_feature_importance['importance'].cumsum()\n",
    "\n",
    "for thresh in [0.75,0.8,0.85,0.9,0.95,1.0]:\n",
    "  shortlist = tree_feature_importance[tree_feature_importance['cumulative'] <= thresh]\n",
    "  print(\"shortlisting top\", thresh*100, \"% features...\")\n",
    "  df_tree_imp = df[['code'] + list(shortlist['tree_feature'])]\n",
    "  dum_clf = vanilla_Dt_classf(df_tree_imp)\n",
    "  print(\"..............\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "h09nbfq0y4wA",
    "outputId": "132dcf1f-a720-4e77-8de0-9d38ee27166a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>tree_feature</th>\n",
       "      <th>importance</th>\n",
       "      <th>cumulative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>60</td>\n",
       "      <td>fft32</td>\n",
       "      <td>0.000792</td>\n",
       "      <td>0.984421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>74</td>\n",
       "      <td>fft46</td>\n",
       "      <td>0.000722</td>\n",
       "      <td>0.985142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>85</td>\n",
       "      <td>geom7</td>\n",
       "      <td>0.000703</td>\n",
       "      <td>0.985845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>55</td>\n",
       "      <td>fft27</td>\n",
       "      <td>0.000669</td>\n",
       "      <td>0.986514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>65</td>\n",
       "      <td>fft37</td>\n",
       "      <td>0.000658</td>\n",
       "      <td>0.987172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>61</td>\n",
       "      <td>fft33</td>\n",
       "      <td>0.000638</td>\n",
       "      <td>0.987811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>29</td>\n",
       "      <td>fft1</td>\n",
       "      <td>0.000613</td>\n",
       "      <td>0.988424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>111</td>\n",
       "      <td>geom33</td>\n",
       "      <td>0.000595</td>\n",
       "      <td>0.989019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>109</td>\n",
       "      <td>geom31</td>\n",
       "      <td>0.000582</td>\n",
       "      <td>0.989601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>39</td>\n",
       "      <td>fft11</td>\n",
       "      <td>0.000569</td>\n",
       "      <td>0.990170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>124</td>\n",
       "      <td>geom46</td>\n",
       "      <td>0.000550</td>\n",
       "      <td>0.990720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>82</td>\n",
       "      <td>geom4</td>\n",
       "      <td>0.000534</td>\n",
       "      <td>0.991254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>34</td>\n",
       "      <td>fft6</td>\n",
       "      <td>0.000531</td>\n",
       "      <td>0.991786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>54</td>\n",
       "      <td>fft26</td>\n",
       "      <td>0.000511</td>\n",
       "      <td>0.992297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>57</td>\n",
       "      <td>fft29</td>\n",
       "      <td>0.000503</td>\n",
       "      <td>0.992800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>43</td>\n",
       "      <td>fft15</td>\n",
       "      <td>0.000478</td>\n",
       "      <td>0.993278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>128</td>\n",
       "      <td>geom50</td>\n",
       "      <td>0.000466</td>\n",
       "      <td>0.993744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>51</td>\n",
       "      <td>fft23</td>\n",
       "      <td>0.000460</td>\n",
       "      <td>0.994204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>68</td>\n",
       "      <td>fft40</td>\n",
       "      <td>0.000447</td>\n",
       "      <td>0.994651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>72</td>\n",
       "      <td>fft44</td>\n",
       "      <td>0.000382</td>\n",
       "      <td>0.995033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>87</td>\n",
       "      <td>geom9</td>\n",
       "      <td>0.000347</td>\n",
       "      <td>0.995380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>66</td>\n",
       "      <td>fft38</td>\n",
       "      <td>0.000344</td>\n",
       "      <td>0.995724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>32</td>\n",
       "      <td>fft4</td>\n",
       "      <td>0.000344</td>\n",
       "      <td>0.996068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>73</td>\n",
       "      <td>fft45</td>\n",
       "      <td>0.000339</td>\n",
       "      <td>0.996407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>104</td>\n",
       "      <td>geom26</td>\n",
       "      <td>0.000314</td>\n",
       "      <td>0.996721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>67</td>\n",
       "      <td>fft39</td>\n",
       "      <td>0.000296</td>\n",
       "      <td>0.997017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>31</td>\n",
       "      <td>fft3</td>\n",
       "      <td>0.000295</td>\n",
       "      <td>0.997312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>70</td>\n",
       "      <td>fft42</td>\n",
       "      <td>0.000290</td>\n",
       "      <td>0.997602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>45</td>\n",
       "      <td>fft17</td>\n",
       "      <td>0.000290</td>\n",
       "      <td>0.997892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>40</td>\n",
       "      <td>fft12</td>\n",
       "      <td>0.000273</td>\n",
       "      <td>0.998166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>4</td>\n",
       "      <td>leaflet_desc3</td>\n",
       "      <td>0.000273</td>\n",
       "      <td>0.998439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>50</td>\n",
       "      <td>fft22</td>\n",
       "      <td>0.000258</td>\n",
       "      <td>0.998697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>48</td>\n",
       "      <td>fft20</td>\n",
       "      <td>0.000258</td>\n",
       "      <td>0.998955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>23</td>\n",
       "      <td>color7</td>\n",
       "      <td>0.000237</td>\n",
       "      <td>0.999192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>38</td>\n",
       "      <td>fft10</td>\n",
       "      <td>0.000228</td>\n",
       "      <td>0.999419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>58</td>\n",
       "      <td>fft30</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>0.999591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>52</td>\n",
       "      <td>fft24</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.999710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>36</td>\n",
       "      <td>fft8</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.999828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>123</td>\n",
       "      <td>geom45</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.999914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>64</td>\n",
       "      <td>fft36</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>76</td>\n",
       "      <td>fft48</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>27</td>\n",
       "      <td>color11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>25</td>\n",
       "      <td>color9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>122</td>\n",
       "      <td>geom44</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>110</td>\n",
       "      <td>geom32</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>24</td>\n",
       "      <td>color8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>56</td>\n",
       "      <td>fft28</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>22</td>\n",
       "      <td>color6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>63</td>\n",
       "      <td>fft35</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>49</td>\n",
       "      <td>fft21</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     index   tree_feature  importance  cumulative\n",
       "79      60          fft32    0.000792    0.984421\n",
       "80      74          fft46    0.000722    0.985142\n",
       "81      85          geom7    0.000703    0.985845\n",
       "82      55          fft27    0.000669    0.986514\n",
       "83      65          fft37    0.000658    0.987172\n",
       "84      61          fft33    0.000638    0.987811\n",
       "85      29           fft1    0.000613    0.988424\n",
       "86     111         geom33    0.000595    0.989019\n",
       "87     109         geom31    0.000582    0.989601\n",
       "88      39          fft11    0.000569    0.990170\n",
       "89     124         geom46    0.000550    0.990720\n",
       "90      82          geom4    0.000534    0.991254\n",
       "91      34           fft6    0.000531    0.991786\n",
       "92      54          fft26    0.000511    0.992297\n",
       "93      57          fft29    0.000503    0.992800\n",
       "94      43          fft15    0.000478    0.993278\n",
       "95     128         geom50    0.000466    0.993744\n",
       "96      51          fft23    0.000460    0.994204\n",
       "97      68          fft40    0.000447    0.994651\n",
       "98      72          fft44    0.000382    0.995033\n",
       "99      87          geom9    0.000347    0.995380\n",
       "100     66          fft38    0.000344    0.995724\n",
       "101     32           fft4    0.000344    0.996068\n",
       "102     73          fft45    0.000339    0.996407\n",
       "103    104         geom26    0.000314    0.996721\n",
       "104     67          fft39    0.000296    0.997017\n",
       "105     31           fft3    0.000295    0.997312\n",
       "106     70          fft42    0.000290    0.997602\n",
       "107     45          fft17    0.000290    0.997892\n",
       "108     40          fft12    0.000273    0.998166\n",
       "109      4  leaflet_desc3    0.000273    0.998439\n",
       "110     50          fft22    0.000258    0.998697\n",
       "111     48          fft20    0.000258    0.998955\n",
       "112     23         color7    0.000237    0.999192\n",
       "113     38          fft10    0.000228    0.999419\n",
       "114     58          fft30    0.000172    0.999591\n",
       "115     52          fft24    0.000118    0.999710\n",
       "116     36           fft8    0.000118    0.999828\n",
       "117    123         geom45    0.000086    0.999914\n",
       "118     64          fft36    0.000086    1.000000\n",
       "119     76          fft48    0.000000    1.000000\n",
       "120     27        color11    0.000000    1.000000\n",
       "121     25         color9    0.000000    1.000000\n",
       "122    122         geom44    0.000000    1.000000\n",
       "123    110         geom32    0.000000    1.000000\n",
       "124     24         color8    0.000000    1.000000\n",
       "125     56          fft28    0.000000    1.000000\n",
       "126     22         color6    0.000000    1.000000\n",
       "127     63          fft35    0.000000    1.000000\n",
       "128     49          fft21    0.000000    1.000000"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_feature_importance.tail(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "AWZ-eRt-QVha",
    "outputId": "4f072699-cc6d-4cf0-ef42-b1447cd7f4c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best threshold (out of 1.0): 0.9\n"
     ]
    }
   ],
   "source": [
    "thresh_final = float(input(\"best threshold (out of 1.0): \"))\n",
    "\n",
    "shortlist = tree_feature_importance[tree_feature_importance['cumulative'] <= thresh_final]\n",
    "df_tree = df[['code'] + list(shortlist['tree_feature'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "id": "7L0uk7F36gWs",
    "outputId": "db880dad-5e81-4964-be8c-52cf38e2697a"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD4CAYAAADlwTGnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAXqklEQVR4nO3df5BdZ2He8e9TLZKJE2QibxgiCVYZqWFkUxLYyO7UMA2OiRwI60zkQa4Hu1M1SgZrmjbNtOth0BAPmUFtJ24ZVFoFGYQaIlM1LttYIAIik0mmVrQCgy07atayUq1w6rWtCAwjhODpH/dduFzf1T272t17757nM7Oz57znPe++5+zuee75LdtERET9/L1udyAiIrojARARUVMJgIiImkoARETUVAIgIqKmBrrdgdm49tprPTQ01O1uRET0lePHjz9ne7C1vK8CYGhoiPHx8W53IyKir0j6m3blOQQUEVFTCYCIiJpKAERE1FQCICKiphIAERE1lQCIiKipBEBERE0lACIiaioBEBFRUwmAiLgiQ6MPMzT6cLe7EXOQAIiIqKkEQERETSUAIiJqqlIASNos6aSkCUmjbaavkPRgmX5U0lDL9NdIelHSb1dtMyIiFlbHAJC0DNgN3ApsBO6QtLGl2jbgnO31wP3Arpbpvwd8ZpZtRkTEAqqyB7AJmLB9yvZF4AAw0lJnBNhXhg8CN0sSgKTbgKeBE7NsMyIiFlCVAFgNnGkanyxlbevYvgScB1ZJ+lHg3wK/M4c2AZC0XdK4pPGpqakK3Y2IiCoW+iTw+4H7bb841wZs77E9bHt4cPAlbzSLiIg5qvJKyLPA2qbxNaWsXZ1JSQPASuB54AZgi6R/B1wDfE/SBeB4hTYjImIBVQmAY8AGSetobKS3Av+kpc4YcDfwv4EtwBHbBt48XUHS+4EXbX+4hESnNiMiYgF1DADblyTtAA4Dy4AHbJ+QdB8wbnsM2AvslzQBvEBjgz7rNq9wWSIiYhaq7AFg+xBwqKVsZ9PwBeD2Dm28v1ObERGxeHIncERETSUAIiJqKgEQEVFTCYCIiJpKAERE1FQCICKiphIAERE1lQCIiKipBEBERE0lACIiaioBEBFRUwmAiIiaSgBERHQwNPowQ6MPd7sb8y4BEBFRUwmAiIiaSgBERNRUpQCQtFnSSUkTkkbbTF8h6cEy/aikoVK+SdKj5esrkn6laZ7Tkh4r08bna4EiIqKajm8Ek7QM2A3cAkwCxySN2X6iqdo24Jzt9ZK2AruAdwGPA8PlFZCvBr4i6X/ZvlTm+3nbz83nAkVERDVV9gA2ARO2T9m+CBwARlrqjAD7yvBB4GZJsv2tpo39VYDno9MR0b+W6hU1/ahKAKwGzjSNT5aytnXKBv88sApA0g2STgCPAb/RFAgGPifpuKTtM/1wSdsljUsan5qaqrJMERFRwYKfBLZ91PZ1wM8B90q6qky6yfYbgVuBeyS9ZYb599getj08ODi40N2NiKiNKgFwFljbNL6mlLWtI2kAWAk831zB9pPAi8D1Zfxs+f4s8BCNQ00REbFIqgTAMWCDpHWSlgNbgbGWOmPA3WV4C3DEtss8AwCSXgu8Djgt6WpJP1bKrwbeRuOEcURELJKOVwGVK3h2AIeBZcADtk9Iug8Ytz0G7AX2S5oAXqAREgA3AaOSvgN8D3iP7eck/RTwkKTpPnzS9mfne+EiImJmHQMAwPYh4FBL2c6m4QvA7W3m2w/sb1N+CnjDbDsbERHzJ3cCR0TUVAIgIqKmEgARETWVAIiIqKkEQERETSUAIiJqKgEQEVFTCYCIiJpKAERE1FQCICKiphIAERE1lQCIiKipBEBERE0lACIiaioBEBFRUwmAiIiaqhQAkjZLOilpQtJom+krJD1Yph+VNFTKN0l6tHx9RdKvVG0zIiIWVscAkLQM2A3cCmwE7pC0saXaNuCc7fXA/cCuUv44MGz7Z4DNwH+VNFCxzYiIWEBV9gA2ARO2T9m+CBwARlrqjAD7yvBB4GZJsv0t25dK+VWAZ9FmREQsoCoBsBo40zQ+Wcra1ikb/PPAKgBJN0g6ATwG/EaZXqVNyvzbJY1LGp+amqrQ3YiIuRsafZih0Ye73Y1FseAngW0ftX0d8HPAvZKumuX8e2wP2x4eHBxcmE5GRNRQlQA4C6xtGl9TytrWkTQArASeb65g+0ngReD6im1GRMQCqhIAx4ANktZJWg5sBcZa6owBd5fhLcAR2y7zDABIei3wOuB0xTZjAdRp9zYiLm+gUwXblyTtAA4Dy4AHbJ+QdB8wbnsM2AvslzQBvEBjgw5wEzAq6TvA94D32H4OoF2b87xsERFxGR0DAMD2IeBQS9nOpuELwO1t5tsP7K/aZkRELJ7cCRwRUVMJgIiImkoARETUVAIgIqKmEgARETWVAIiIqKkEQE3lhrCISABERNRUAiAioqYSABERNZUAiIioqQRARERNJQAiImoqARARUVMJgIiImkoARETUVKUAkLRZ0klJE5JG20xfIenBMv2opKFSfouk45IeK9/f2jTPn5Y2Hy1fPzFfCxUREZ11fCOYpGXAbuAWYBI4JmnM9hNN1bYB52yvl7QV2AW8C3gO+GXbX5N0PY1XQK5umu9O2+PztCwR0UXTjxY5/cG3d7knUVWVPYBNwITtU7YvAgeAkZY6I8C+MnwQuFmSbH/Z9tdK+Qng5ZJWzEfHIyLiylQJgNXAmabxSX74U/wP1bF9CTgPrGqp86vAl2x/u6nsY+Xwz/skqd0Pl7Rd0rik8ampqQrdjYiIKhblJLCk62gcFvr1puI7bb8eeHP5ene7eW3vsT1se3hwcHDhOxsRURNVAuAssLZpfE0pa1tH0gCwEni+jK8BHgLusv3U9Ay2z5bv3wA+SeNQU0RELJIqAXAM2CBpnaTlwFZgrKXOGHB3Gd4CHLFtSdcADwOjtv9iurKkAUnXluGXAe8AHr+yRYmIiNnoGADlmP4OGlfwPAl8yvYJSfdJemepthdYJWkC+C1g+lLRHcB6YGfL5Z4rgMOSvgo8SmMP4vfnc8EiIuLyOl4GCmD7EHCopWxn0/AF4PY2830A+MAMzb6pejcjImK+5U7giIiaSgBERNRUAiAioo2h0Ye/f3fzUpUAiIioqQRARERNJQAiImoqAVBRHY4HRkS9JAAiImoqARARUVMJgIiImkoARETUVAIgIqKmEgARETWVAIiIqKkEQERETdUuAHJDV0REQ6UAkLRZ0klJE5JG20xfIenBMv2opKFSfouk45IeK9/f2jTPm0r5hKQPSdJ8LVRERHTWMQAkLQN2A7cCG4E7JG1sqbYNOGd7PXA/sKuUPwf8su3X03hn8P6meT4C/BqwoXxtvoLliIiIWaqyB7AJmLB9yvZF4AAw0lJnBNhXhg8CN0uS7S/b/lopPwG8vOwtvBp4he1HbBv4BHDbFS9NRERUViUAVgNnmsYnS1nbOuUl8ueBVS11fhX4ku1vl/qTHdoEQNJ2SeOSxqempip0NyKWspzHmz+LchJY0nU0Dgv9+mzntb3H9rDt4cHBwfnvXER0VTbo3VMlAM4Ca5vG15SytnUkDQArgefL+BrgIeAu20811V/Toc2IiFhAVQLgGLBB0jpJy4GtwFhLnTEaJ3kBtgBHbFvSNcDDwKjtv5iubPsZ4OuSbixX/9wFfPoKlyUiImahYwCUY/o7gMPAk8CnbJ+QdJ+kd5Zqe4FVkiaA3wKmLxXdAawHdkp6tHz9RJn2HuCjwATwFPCZ+VqoiIjobKBKJduHgEMtZTubhi8At7eZ7wPAB2Zocxy4fjadjYiI+VO7O4EjIqKh0h5ARMR8y5U/3Zc9gBnk0rSIWOoSABERNZUAiIioqQRARERNJQAiImoqARARUVMJgIiImkoARETUVAIgInpa7sdZOAmAiIiaSgBERNRUAiAioqYSABERNZWngUZEX8jJ4PlXaQ9A0mZJJyVNSBptM32FpAfL9KOShkr5KklflPSipA+3zPOnpc3WN4VFRMQi6LgHIGkZsBu4BZgEjkkas/1EU7VtwDnb6yVtBXYB7wIuAO+j8eavdm//urO8Gaxn5VNHxA/+D05/8O1d7knMpyp7AJuACdunbF8EDgAjLXVGgH1l+CBwsyTZ/qbtP6cRBBGxhOTDUf+rEgCrgTNN45OlrG2d8hL588CqCm1/rBz+eZ8ktasgabukcUnjU1NTFZqMiIgqunkV0J22Xw+8uXy9u10l23tsD9seHhwcXNQORkQsZVUC4Cywtml8TSlrW0fSALASeP5yjdo+W75/A/gkjUNNERGxSKoEwDFgg6R1kpYDW4GxljpjwN1leAtwxLZnalDSgKRry/DLgHcAj8+28xHRH/KO7d7U8Sog25ck7QAOA8uAB2yfkHQfMG57DNgL7Jc0AbxAIyQAkHQaeAWwXNJtwNuAvwEOl43/MuDzwO/P65JFRMRlVboRzPYh4FBL2c6m4QvA7TPMOzRDs2+q1sWIiFgIeRRERMQC6fVDXwmAJlV/WXP9pfb6H0NE1EsCICKiphIAVyCf6KNXLLW/xaW2PL0qAdAj8gcfEYstARARUVN5HwB5qFVE1FP2ACKishyqXFoSABERNZVDQBHRM7J3sbiyBzBPZvOHm93oiOgFCYCIiEXUSx/+EgALJJ/yI6LX1eYcwGJtjLPRj4h+kT2AiOh7vbTH3Ut96SQBEBFRU5UCQNJmSSclTUgabTN9haQHy/SjkoZK+SpJX5T0oqQPt8zzJkmPlXk+JEnzsUAREd3QT5/8p3UMAEnLgN3ArcBG4A5JG1uqbQPO2V4P3A/sKuUXgPcBv92m6Y8AvwZsKF+b57IAERExN1X2ADYBE7ZP2b4IHABGWuqMAPvK8EHgZkmy/U3bf04jCL5P0quBV9h+pLw8/hPAbVeyIL2sHz8ZREAualjqqlwFtBo40zQ+CdwwU53yEvnzwCrgucu0OdnS5up2FSVtB7YDvOY1r6nQ3YjotungOP3Bt3e5Jwuvn0Oy508C295je9j28ODgYLe7ExGxZFQJgLPA2qbxNaWsbR1JA8BK4PkOba7p0GZERCygKgFwDNggaZ2k5cBWYKylzhhwdxneAhwpx/bbsv0M8HVJN5arf+4CPj3r3kdExJx1PAdQjunvAA4Dy4AHbJ+QdB8wbnsM2AvslzQBvEAjJACQdBp4BbBc0m3A22w/AbwH+DjwcuAz5SsiIhZJpUdB2D4EHGop29k0fAG4fYZ5h2YoHweur9rRiIhuW2ont2vzLKBe1c9XEEREf+v5q4AiInrNUrm3JwEQEVFTtT4EtBQSPCJirrIH0MOWym5mRMysm//nCYCIJSYfGvpDL3zASwBERNRUAiAiokcs9l5BAiAioqYSABER1PPcSQIgIoDeOCkZiysBEN+XDUBEvSQAIuKH5INAfdT6TuCIumndsC+Vp1rG3GQPIGKJqvJJPp/26y0BEBFRU5UCQNJmSSclTUgabTN9haQHy/Sjkoaapt1byk9K+sWm8tOSHpP0qKTx+ViYpaDXPpH1Wn8iYv50PAcgaRmwG7gFmASOSRorr3Wctg04Z3u9pK3ALuBdkjbSeD3kdcBPAp+X9Pdtf7fM9/O2n5vH5Yk5yAY+op6q7AFsAiZsn7J9ETgAjLTUGQH2leGDwM3lZe8jwAHb37b9NDBR2ouIiC6rEgCrgTNN45OlrG0d25eA88CqDvMa+Jyk45K2z/TDJW2XNC5pfGpqqkJ3IyKiim6eBL7J9huBW4F7JL2lXSXbe2wP2x4eHBxc3B5GLAE5jxMzqXIfwFlgbdP4mlLWrs6kpAFgJfD85ea1Pf39WUkP0Tg09GdzWIaI6FEJnt5WZQ/gGLBB0jpJy2mc1B1rqTMG3F2GtwBHbLuUby1XCa0DNgB/KelqST8GIOlq4G3A41e+OBERvadX98I67gHYviRpB3AYWAY8YPuEpPuAcdtjwF5gv6QJ4AUaIUGp9yngCeAScI/t70p6FfBQ4zwxA8AnbX92AZYvIpawXtyo9pNKj4KwfQg41FK2s2n4AnD7DPP+LvC7LWWngDfMtrMRETF/cidwRCwZvXqopVclACIiaioB0AfyqSYiFkICINpK4EQsfQmAiIges1h7/XkhTEQsSXn5TWfZA4hKch4iYulJAMSstYZBwiGiP+UQUMxZNvoR/S17AH1mNhvdfDKPiMtJAERcoQRt9KscAooFMb1BnO2VF3Odbz60+9lzvZJkPpcjV7PEQkkA9Klubigvp3VjVbWf7T5BX27euU5rV2c2fb7cfNPTZ7usM7U1Ux+af072POJKJABi0cy04ZvLBnM205p/zmJvMNv1Ibrjcnt4df295BxARERNZQ8gImqnXw6dLfQeSqU9AEmbJZ2UNCFptM30FZIeLNOPShpqmnZvKT8p6RerthkREQurYwBIWgbsBm4FNgJ3SNrYUm0bcM72euB+YFeZdyON10NeB2wG/rOkZRXbjApyCWJEzFWVQ0CbgInyGkckHQBGaLznd9oI8P4yfBD4sBov/B0BDtj+NvB0eWfwplKvU5sxSwmCiJgN2b58BWkLsNn2Py/j7wZusL2jqc7jpc5kGX8KuIFGKDxi+7+V8r3AZ8psl22zqe3twPYy+tPAybktKgDXAs9dwfzdlL53Tz/3P33vjl7r+2ttD7YW9vxJYNt7gD3z0ZakcdvD89HWYkvfu6ef+5++d0e/9L3KSeCzwNqm8TWlrG0dSQPASuD5y8xbpc2IiFhAVQLgGLBB0jpJy2mc1B1rqTMG3F2GtwBH3Di2NAZsLVcJrQM2AH9Zsc2IiFhAHQ8B2b4kaQdwGFgGPGD7hKT7gHHbY8BeYH85yfsCjQ06pd6naJzcvQTcY/u7AO3anP/Fe4l5OZTUJel79/Rz/9P37uiLvnc8CRwREUtTHgUREVFTCYCIiJqqRQD022MnJK2V9EVJT0g6Iek3S/mPS/oTSX9dvr+y232dSbnj+8uS/riMryuPCZkojw1Z3u0+tiPpGkkHJf2VpCcl/cN+We+S/lX5e3lc0h9KuqqX17ukByQ9W+4jmi5ru67V8KGyHF+V9Mbu9XzGvv/78nfzVUkPSbqmaVrbR+J025IPgD597MQl4F/b3gjcCNxT+jwKfMH2BuALZbxX/SbwZNP4LuD+8riQczQeH9KL/hPwWduvA95AYxl6fr1LWg38C2DY9vU0Lq7YSm+v94/TeERMs5nW9a00riLcQOPG0I8sUh9n8nFe2vc/Aa63/Q+A/wPcCzM/EmfxujqzJR8AND3KwvZFYPqxEz3L9jO2v1SGv0FjI7SaRr/3lWr7gNu608PLk7QGeDvw0TIu4K00HhMCPdp3SSuBt9C4qg3bF23/HX2y3mlc1ffyci/OjwDP0MPr3faf0bhqsNlM63oE+IQbHgGukfTqxenpS7Xru+3P2b5URh+hcX8TND0Sx/bTQPMjcbqqDgGwGjjTND5ZyvqCGk9W/VngKPAq28+USX8LvKpL3erkPwL/BvheGV8F/F3TP0ev/g7WAVPAx8rhq49Kupo+WO+2zwL/Afi/NDb854Hj9Md6bzbTuu63/+N/xg8ee9Ozfa9DAPQtST8K/A/gX9r+evO0cqNdz13DK+kdwLO2j3e7L3MwALwR+IjtnwW+Scvhnh5e76+k8UlzHfCTwNW89BBFX+nVdd2JpPfSOIz7B93uSyd1CIC+fOyEpJfR2Pj/ge0/KsX/b3q3t3x/tlv9u4x/BLxT0mkah9veSuO4+jXl0AT07u9gEpi0fbSMH6QRCP2w3n8BeNr2lO3vAH9E43fRD+u92Uzrui/+jyX9U+AdwJ3+wU1WPdv3OgRA3z12ohwz3ws8afv3miY1P3LjbuDTi923Tmzfa3uN7SEa6/qI7TuBL9J4TAj0bt//Fjgj6adL0c007mLv+fVO49DPjZJ+pPz9TPe959d7i5nW9RhwV7ka6EbgfNOhop4gaTONQ5/vtP2tpkkzPRKn+2wv+S/gl2iclX8KeG+3+1OhvzfR2PX9KvBo+folGsfSvwD8NfB54Me73dcOy/GPgT8uwz9F449+AvjvwIpu92+GPv8MMF7W/f8EXtkv6x34HeCvgMeB/cCKXl7vwB/SOF/xHRp7X9tmWteAaFzN9xTwGI2rnXqt7xM0jvVP/8/+l6b67y19Pwnc2u11P/2VR0FERNRUHQ4BRUREGwmAiIiaSgBERNRUAiAioqYSABERNZUAiIioqQRARERN/X+cRvxqM1tdPQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# feature importance - random forest\n",
    "\n",
    "importance = rf_clf.feature_importances_\n",
    "# summarize feature importance\n",
    "# for i,v in enumerate(importance):\n",
    "\t# print('Feature: %0d, Score: %.5f' % (i,v))\n",
    "# plot feature importance\n",
    "plt.bar([x for x in range(len(importance))], importance)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 344
    },
    "colab_type": "code",
    "id": "zXVFSPT0Bmr8",
    "outputId": "3e3f7bb3-7969-492b-fca7-6b38010a1716"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shortlisting top 75.0 % features...\n",
      "random forest classifier: 0.9160748870238864\n",
      "..............\n",
      "shortlisting top 80.0 % features...\n",
      "random forest classifier: 0.9302775984506133\n",
      "..............\n",
      "shortlisting top 85.0 % features...\n",
      "random forest classifier: 0.9380245319561007\n",
      "..............\n",
      "shortlisting top 90.0 % features...\n",
      "random forest classifier: 0.9367333763718528\n",
      "..............\n",
      "shortlisting top 95.0 % features...\n",
      "random forest classifier: 0.9309231762427372\n",
      "..............\n",
      "shortlisting top 100.0 % features...\n",
      "random forest classifier: 0.9173660426081343\n",
      "..............\n"
     ]
    }
   ],
   "source": [
    "# shortlist x% cumulative importance\n",
    "\n",
    "rf_feature_importance = pd.DataFrame({'rf_feature':list(df.columns[1:]),'importance':list(importance)})\n",
    "rf_feature_importance = rf_feature_importance.sort_values('importance', ascending=False).reset_index()\n",
    "rf_feature_importance['cumulative'] = rf_feature_importance['importance'].cumsum()\n",
    "\n",
    "for thresh in [0.75,0.8,0.85,0.9,0.95,1.0]:\n",
    "  shortlist = rf_feature_importance[rf_feature_importance['cumulative'] <= thresh]\n",
    "  print(\"shortlisting top\", thresh*100, \"% features...\")\n",
    "  df_rf_imp = df[['code'] + list(shortlist['rf_feature'])]\n",
    "  dum_clf = vanilla_Rf_classf(df_rf_imp)\n",
    "  print(\"..............\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "oN146c5jzeqG",
    "outputId": "dc33312a-ced7-4215-cd52-6b1363ab5860"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>rf_feature</th>\n",
       "      <th>importance</th>\n",
       "      <th>cumulative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>26</td>\n",
       "      <td>color10</td>\n",
       "      <td>0.001918</td>\n",
       "      <td>0.940162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>64</td>\n",
       "      <td>fft36</td>\n",
       "      <td>0.001696</td>\n",
       "      <td>0.941858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>45</td>\n",
       "      <td>fft17</td>\n",
       "      <td>0.001585</td>\n",
       "      <td>0.943443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>74</td>\n",
       "      <td>fft46</td>\n",
       "      <td>0.001520</td>\n",
       "      <td>0.944963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>66</td>\n",
       "      <td>fft38</td>\n",
       "      <td>0.001502</td>\n",
       "      <td>0.946465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>61</td>\n",
       "      <td>fft33</td>\n",
       "      <td>0.001490</td>\n",
       "      <td>0.947955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>39</td>\n",
       "      <td>fft11</td>\n",
       "      <td>0.001485</td>\n",
       "      <td>0.949441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>58</td>\n",
       "      <td>fft30</td>\n",
       "      <td>0.001483</td>\n",
       "      <td>0.950923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>75</td>\n",
       "      <td>fft47</td>\n",
       "      <td>0.001481</td>\n",
       "      <td>0.952404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>60</td>\n",
       "      <td>fft32</td>\n",
       "      <td>0.001470</td>\n",
       "      <td>0.953874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>65</td>\n",
       "      <td>fft37</td>\n",
       "      <td>0.001461</td>\n",
       "      <td>0.955334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>56</td>\n",
       "      <td>fft28</td>\n",
       "      <td>0.001454</td>\n",
       "      <td>0.956788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>57</td>\n",
       "      <td>fft29</td>\n",
       "      <td>0.001442</td>\n",
       "      <td>0.958230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>72</td>\n",
       "      <td>fft44</td>\n",
       "      <td>0.001439</td>\n",
       "      <td>0.959669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>46</td>\n",
       "      <td>fft18</td>\n",
       "      <td>0.001419</td>\n",
       "      <td>0.961088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>77</td>\n",
       "      <td>fft49</td>\n",
       "      <td>0.001417</td>\n",
       "      <td>0.962505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>38</td>\n",
       "      <td>fft10</td>\n",
       "      <td>0.001402</td>\n",
       "      <td>0.963907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>59</td>\n",
       "      <td>fft31</td>\n",
       "      <td>0.001383</td>\n",
       "      <td>0.965290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>44</td>\n",
       "      <td>fft16</td>\n",
       "      <td>0.001381</td>\n",
       "      <td>0.966671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>71</td>\n",
       "      <td>fft43</td>\n",
       "      <td>0.001375</td>\n",
       "      <td>0.968046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>40</td>\n",
       "      <td>fft12</td>\n",
       "      <td>0.001372</td>\n",
       "      <td>0.969418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>52</td>\n",
       "      <td>fft24</td>\n",
       "      <td>0.001358</td>\n",
       "      <td>0.970776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>69</td>\n",
       "      <td>fft41</td>\n",
       "      <td>0.001352</td>\n",
       "      <td>0.972128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>22</td>\n",
       "      <td>color6</td>\n",
       "      <td>0.001351</td>\n",
       "      <td>0.973479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>49</td>\n",
       "      <td>fft21</td>\n",
       "      <td>0.001322</td>\n",
       "      <td>0.974801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>63</td>\n",
       "      <td>fft35</td>\n",
       "      <td>0.001305</td>\n",
       "      <td>0.976106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>62</td>\n",
       "      <td>fft34</td>\n",
       "      <td>0.001295</td>\n",
       "      <td>0.977401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>70</td>\n",
       "      <td>fft42</td>\n",
       "      <td>0.001287</td>\n",
       "      <td>0.978688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>73</td>\n",
       "      <td>fft45</td>\n",
       "      <td>0.001266</td>\n",
       "      <td>0.979954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>76</td>\n",
       "      <td>fft48</td>\n",
       "      <td>0.001264</td>\n",
       "      <td>0.981218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>47</td>\n",
       "      <td>fft19</td>\n",
       "      <td>0.001244</td>\n",
       "      <td>0.982463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>54</td>\n",
       "      <td>fft26</td>\n",
       "      <td>0.001243</td>\n",
       "      <td>0.983706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>55</td>\n",
       "      <td>fft27</td>\n",
       "      <td>0.001230</td>\n",
       "      <td>0.984936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>43</td>\n",
       "      <td>fft15</td>\n",
       "      <td>0.001227</td>\n",
       "      <td>0.986163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>68</td>\n",
       "      <td>fft40</td>\n",
       "      <td>0.001211</td>\n",
       "      <td>0.987374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>48</td>\n",
       "      <td>fft20</td>\n",
       "      <td>0.001207</td>\n",
       "      <td>0.988581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>67</td>\n",
       "      <td>fft39</td>\n",
       "      <td>0.001204</td>\n",
       "      <td>0.989785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>41</td>\n",
       "      <td>fft13</td>\n",
       "      <td>0.001201</td>\n",
       "      <td>0.990986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>53</td>\n",
       "      <td>fft25</td>\n",
       "      <td>0.001194</td>\n",
       "      <td>0.992180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>111</td>\n",
       "      <td>geom33</td>\n",
       "      <td>0.001191</td>\n",
       "      <td>0.993371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>42</td>\n",
       "      <td>fft14</td>\n",
       "      <td>0.001169</td>\n",
       "      <td>0.994540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>50</td>\n",
       "      <td>fft22</td>\n",
       "      <td>0.001146</td>\n",
       "      <td>0.995686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>51</td>\n",
       "      <td>fft23</td>\n",
       "      <td>0.001145</td>\n",
       "      <td>0.996831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>109</td>\n",
       "      <td>geom31</td>\n",
       "      <td>0.001074</td>\n",
       "      <td>0.997905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>128</td>\n",
       "      <td>geom50</td>\n",
       "      <td>0.000850</td>\n",
       "      <td>0.998755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>110</td>\n",
       "      <td>geom32</td>\n",
       "      <td>0.000691</td>\n",
       "      <td>0.999446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>23</td>\n",
       "      <td>color7</td>\n",
       "      <td>0.000358</td>\n",
       "      <td>0.999804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>25</td>\n",
       "      <td>color9</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.999929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>24</td>\n",
       "      <td>color8</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>27</td>\n",
       "      <td>color11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     index rf_feature  importance  cumulative\n",
       "79      26    color10    0.001918    0.940162\n",
       "80      64      fft36    0.001696    0.941858\n",
       "81      45      fft17    0.001585    0.943443\n",
       "82      74      fft46    0.001520    0.944963\n",
       "83      66      fft38    0.001502    0.946465\n",
       "84      61      fft33    0.001490    0.947955\n",
       "85      39      fft11    0.001485    0.949441\n",
       "86      58      fft30    0.001483    0.950923\n",
       "87      75      fft47    0.001481    0.952404\n",
       "88      60      fft32    0.001470    0.953874\n",
       "89      65      fft37    0.001461    0.955334\n",
       "90      56      fft28    0.001454    0.956788\n",
       "91      57      fft29    0.001442    0.958230\n",
       "92      72      fft44    0.001439    0.959669\n",
       "93      46      fft18    0.001419    0.961088\n",
       "94      77      fft49    0.001417    0.962505\n",
       "95      38      fft10    0.001402    0.963907\n",
       "96      59      fft31    0.001383    0.965290\n",
       "97      44      fft16    0.001381    0.966671\n",
       "98      71      fft43    0.001375    0.968046\n",
       "99      40      fft12    0.001372    0.969418\n",
       "100     52      fft24    0.001358    0.970776\n",
       "101     69      fft41    0.001352    0.972128\n",
       "102     22     color6    0.001351    0.973479\n",
       "103     49      fft21    0.001322    0.974801\n",
       "104     63      fft35    0.001305    0.976106\n",
       "105     62      fft34    0.001295    0.977401\n",
       "106     70      fft42    0.001287    0.978688\n",
       "107     73      fft45    0.001266    0.979954\n",
       "108     76      fft48    0.001264    0.981218\n",
       "109     47      fft19    0.001244    0.982463\n",
       "110     54      fft26    0.001243    0.983706\n",
       "111     55      fft27    0.001230    0.984936\n",
       "112     43      fft15    0.001227    0.986163\n",
       "113     68      fft40    0.001211    0.987374\n",
       "114     48      fft20    0.001207    0.988581\n",
       "115     67      fft39    0.001204    0.989785\n",
       "116     41      fft13    0.001201    0.990986\n",
       "117     53      fft25    0.001194    0.992180\n",
       "118    111     geom33    0.001191    0.993371\n",
       "119     42      fft14    0.001169    0.994540\n",
       "120     50      fft22    0.001146    0.995686\n",
       "121     51      fft23    0.001145    0.996831\n",
       "122    109     geom31    0.001074    0.997905\n",
       "123    128     geom50    0.000850    0.998755\n",
       "124    110     geom32    0.000691    0.999446\n",
       "125     23     color7    0.000358    0.999804\n",
       "126     25     color9    0.000125    0.999929\n",
       "127     24     color8    0.000071    1.000000\n",
       "128     27    color11    0.000000    1.000000"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_feature_importance.tail(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "XxhGJfP4KLEi",
    "outputId": "34f22840-cf4b-4709-a742-f01d4962a625"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best threshold (out of 1.0): 0.85\n"
     ]
    }
   ],
   "source": [
    "thresh_final = float(input(\"best threshold (out of 1.0): \"))\n",
    "\n",
    "shortlist = rf_feature_importance[rf_feature_importance['cumulative'] <= thresh_final]\n",
    "df_rf = df[['code'] + list(shortlist['rf_feature'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NtNRcXkVo7we"
   },
   "source": [
    "# hyperparameter tuning: gridsearchCV, for both\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 637
    },
    "colab_type": "code",
    "id": "srwWo9WaM--I",
    "outputId": "dad30522-9029-4db4-a101-52cecbd213e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 360 candidates, totalling 1800 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:667: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:   39.7s\n",
      "[Parallel(n_jobs=-1)]: Done 124 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 284 tasks      | elapsed:  6.4min\n",
      "[Parallel(n_jobs=-1)]: Done 508 tasks      | elapsed: 10.2min\n",
      "[Parallel(n_jobs=-1)]: Done 796 tasks      | elapsed: 11.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1148 tasks      | elapsed: 12.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1564 tasks      | elapsed: 13.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1800 out of 1800 | elapsed: 14.4min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score=nan,\n",
       "             estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,\n",
       "                                              criterion='gini', max_depth=None,\n",
       "                                              max_features=None,\n",
       "                                              max_leaf_nodes=None,\n",
       "                                              min_impurity_decrease=0.0,\n",
       "                                              min_impurity_split=None,\n",
       "                                              min_samples_leaf=1,\n",
       "                                              min_samples_split=2,\n",
       "                                              min_weight_fraction_leaf=0.0,\n",
       "                                              presort='deprecated',\n",
       "                                              random_state=None,\n",
       "                                              splitter='best'),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid={'criterion': ['entropy'],\n",
       "                         'max_features': [None, 'auto', 'sqrt', 'log2'],\n",
       "                         'min_samples_leaf': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
       "                         'min_samples_split': [2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
       "                         'random_state': [23]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=3)"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grid search for decision tree\n",
    "s_plit = [x for x in range(2,11)]\n",
    "l_eaf = [x for x in range(1,11)]\n",
    "\n",
    "params = {'max_features': [None, 'auto', 'sqrt', 'log2'],\n",
    "          'criterion':[\"entropy\"],\n",
    "          'min_samples_split':s_plit, # controls overfitting, also depth of tree\n",
    "          'min_samples_leaf':l_eaf, # controls overfitting, leaves need not have zero entropy\n",
    "          'random_state':[23]}\n",
    "\n",
    "X_train_tree, y_train_tree, X_test_tree, y_test_tree = util_vanilla(df_tree)\n",
    "\n",
    "grid_tree = GridSearchCV(tree.DecisionTreeClassifier(), param_grid=params, verbose=3, n_jobs=-1)\n",
    "grid_tree.fit(X_train_tree, y_train_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 271
    },
    "colab_type": "code",
    "id": "Vc1TcIsSmpYV",
    "outputId": "a8c8c3bd-e136-423f-ce8e-0b0cb1c17c15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best hyperparams:  {'criterion': 'entropy', 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'random_state': 23}\n",
      "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='entropy',\n",
      "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                       random_state=23, splitter='best')\n",
      "\n",
      "\n",
      "Train set\n",
      "Accuracy: 1.0\n",
      ".......................\n",
      "Test set\n",
      "Accuracy: 0.7204648160103292\n"
     ]
    }
   ],
   "source": [
    "print(\"best hyperparams: \", grid_tree.best_params_)\n",
    "print(grid_tree.best_estimator_)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Train set\")\n",
    "tree_pred_train = grid_tree.predict(X_train_tree)\n",
    "print(\"Accuracy:\",accuracy_score(y_train_tree, tree_pred_train))\n",
    "\n",
    "print(\".......................\")\n",
    "\n",
    "print(\"Test set\")\n",
    "tree_pred_test = grid_tree.predict(X_test_tree)\n",
    "print(\"Accuracy:\",accuracy_score(y_test_tree, tree_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "I5rJeY0joodO",
    "outputId": "3e9af62e-fe90-4db8-c039-a78afb258f63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00        39\n",
      "         1.0       1.00      1.00      1.00        48\n",
      "         2.0       1.00      1.00      1.00        30\n",
      "         3.0       1.00      1.00      1.00        23\n",
      "         4.0       1.00      1.00      1.00        21\n",
      "         5.0       1.00      1.00      1.00        10\n",
      "         6.0       1.00      1.00      1.00         8\n",
      "         7.0       1.00      1.00      1.00        15\n",
      "         8.0       1.00      1.00      1.00        36\n",
      "         9.0       1.00      1.00      1.00        32\n",
      "        10.0       1.00      1.00      1.00        16\n",
      "        11.0       1.00      1.00      1.00        14\n",
      "        12.0       1.00      1.00      1.00        46\n",
      "        13.0       1.00      1.00      1.00        50\n",
      "        14.0       1.00      1.00      1.00         3\n",
      "        15.0       1.00      1.00      1.00        11\n",
      "        16.0       1.00      1.00      1.00        41\n",
      "        17.0       1.00      1.00      1.00        53\n",
      "        18.0       1.00      1.00      1.00       132\n",
      "        19.0       1.00      1.00      1.00        18\n",
      "        20.0       1.00      1.00      1.00        40\n",
      "        21.0       1.00      1.00      1.00        41\n",
      "        22.0       1.00      1.00      1.00        52\n",
      "        23.0       1.00      1.00      1.00        34\n",
      "        24.0       1.00      1.00      1.00        43\n",
      "        25.0       1.00      1.00      1.00         7\n",
      "        26.0       1.00      1.00      1.00        27\n",
      "        27.0       1.00      1.00      1.00        18\n",
      "        28.0       1.00      1.00      1.00        42\n",
      "        29.0       1.00      1.00      1.00        62\n",
      "        30.0       1.00      1.00      1.00        27\n",
      "        31.0       1.00      1.00      1.00         5\n",
      "        32.0       1.00      1.00      1.00        43\n",
      "        33.0       1.00      1.00      1.00        66\n",
      "        34.0       1.00      1.00      1.00        41\n",
      "        35.0       1.00      1.00      1.00         1\n",
      "        36.0       1.00      1.00      1.00        25\n",
      "        37.0       1.00      1.00      1.00         7\n",
      "        38.0       1.00      1.00      1.00        80\n",
      "        39.0       1.00      1.00      1.00        26\n",
      "        40.0       1.00      1.00      1.00        41\n",
      "        41.0       1.00      1.00      1.00         5\n",
      "        42.0       1.00      1.00      1.00        50\n",
      "        43.0       1.00      1.00      1.00         4\n",
      "        44.0       1.00      1.00      1.00        15\n",
      "        45.0       1.00      1.00      1.00        48\n",
      "        46.0       1.00      1.00      1.00        22\n",
      "        47.0       1.00      1.00      1.00        64\n",
      "        48.0       1.00      1.00      1.00        30\n",
      "        49.0       1.00      1.00      1.00        15\n",
      "        50.0       1.00      1.00      1.00        14\n",
      "        51.0       1.00      1.00      1.00        57\n",
      "        52.0       1.00      1.00      1.00         3\n",
      "        53.0       1.00      1.00      1.00        21\n",
      "        54.0       1.00      1.00      1.00        29\n",
      "        55.0       1.00      1.00      1.00        18\n",
      "        56.0       1.00      1.00      1.00        11\n",
      "        57.0       1.00      1.00      1.00        12\n",
      "        58.0       1.00      1.00      1.00        22\n",
      "        59.0       1.00      1.00      1.00        12\n",
      "        60.0       1.00      1.00      1.00        29\n",
      "        61.0       1.00      1.00      1.00        14\n",
      "        62.0       1.00      1.00      1.00        25\n",
      "        63.0       1.00      1.00      1.00         1\n",
      "        64.0       1.00      1.00      1.00        69\n",
      "        65.0       1.00      1.00      1.00        99\n",
      "        66.0       1.00      1.00      1.00         3\n",
      "        67.0       1.00      1.00      1.00        35\n",
      "        68.0       1.00      1.00      1.00        32\n",
      "        69.0       1.00      1.00      1.00        25\n",
      "        70.0       1.00      1.00      1.00        54\n",
      "        71.0       1.00      1.00      1.00        25\n",
      "        72.0       1.00      1.00      1.00        54\n",
      "        73.0       1.00      1.00      1.00        47\n",
      "        74.0       1.00      1.00      1.00        41\n",
      "        75.0       1.00      1.00      1.00        46\n",
      "        76.0       1.00      1.00      1.00        90\n",
      "        77.0       1.00      1.00      1.00        25\n",
      "        78.0       1.00      1.00      1.00         1\n",
      "        79.0       1.00      1.00      1.00        29\n",
      "        80.0       1.00      1.00      1.00         7\n",
      "        81.0       1.00      1.00      1.00        27\n",
      "        82.0       1.00      1.00      1.00         3\n",
      "        83.0       1.00      1.00      1.00        17\n",
      "        84.0       1.00      1.00      1.00        11\n",
      "        85.0       1.00      1.00      1.00        27\n",
      "        86.0       1.00      1.00      1.00         7\n",
      "        87.0       1.00      1.00      1.00        28\n",
      "        88.0       1.00      1.00      1.00        92\n",
      "        89.0       1.00      1.00      1.00         7\n",
      "        90.0       1.00      1.00      1.00        23\n",
      "        91.0       1.00      1.00      1.00        38\n",
      "        92.0       1.00      1.00      1.00         8\n",
      "        93.0       1.00      1.00      1.00         8\n",
      "        94.0       1.00      1.00      1.00        56\n",
      "        95.0       1.00      1.00      1.00        19\n",
      "        96.0       1.00      1.00      1.00        65\n",
      "        97.0       1.00      1.00      1.00        38\n",
      "        98.0       1.00      1.00      1.00        37\n",
      "        99.0       1.00      1.00      1.00        64\n",
      "       100.0       1.00      1.00      1.00         5\n",
      "       101.0       1.00      1.00      1.00        18\n",
      "       102.0       1.00      1.00      1.00         1\n",
      "       103.0       1.00      1.00      1.00         1\n",
      "       104.0       1.00      1.00      1.00         6\n",
      "       105.0       1.00      1.00      1.00        28\n",
      "       106.0       1.00      1.00      1.00        14\n",
      "       107.0       1.00      1.00      1.00         3\n",
      "       108.0       1.00      1.00      1.00        11\n",
      "       109.0       1.00      1.00      1.00         1\n",
      "       110.0       1.00      1.00      1.00         3\n",
      "       111.0       1.00      1.00      1.00        15\n",
      "       112.0       1.00      1.00      1.00        46\n",
      "       113.0       1.00      1.00      1.00        16\n",
      "       114.0       1.00      1.00      1.00         8\n",
      "       115.0       1.00      1.00      1.00         1\n",
      "       116.0       1.00      1.00      1.00         2\n",
      "       117.0       1.00      1.00      1.00       136\n",
      "       118.0       1.00      1.00      1.00        12\n",
      "       119.0       1.00      1.00      1.00        16\n",
      "       120.0       1.00      1.00      1.00        79\n",
      "       121.0       1.00      1.00      1.00        40\n",
      "\n",
      "    accuracy                           1.00      3614\n",
      "   macro avg       1.00      1.00      1.00      3614\n",
      "weighted avg       1.00      1.00      1.00      3614\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_train_tree, tree_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ImNhPSpIoxZR",
    "outputId": "c5477ef4-03c1-4da4-920b-40391a6b34ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.47      0.41      0.44        17\n",
      "         1.0       0.64      0.67      0.65        21\n",
      "         2.0       0.67      0.77      0.71        13\n",
      "         3.0       0.50      0.40      0.44        10\n",
      "         4.0       0.57      0.44      0.50         9\n",
      "         5.0       0.67      0.50      0.57         4\n",
      "         6.0       0.25      0.25      0.25         4\n",
      "         7.0       0.33      0.43      0.38         7\n",
      "         8.0       0.80      0.80      0.80        15\n",
      "         9.0       0.92      0.85      0.88        13\n",
      "        10.0       0.40      0.29      0.33         7\n",
      "        11.0       0.33      0.33      0.33         6\n",
      "        12.0       0.60      0.79      0.68        19\n",
      "        13.0       0.88      0.68      0.77        22\n",
      "        14.0       0.00      0.00      0.00         1\n",
      "        15.0       0.12      0.20      0.15         5\n",
      "        16.0       0.76      0.76      0.76        17\n",
      "        17.0       0.78      0.82      0.80        22\n",
      "        18.0       0.96      0.88      0.92        56\n",
      "        19.0       1.00      0.88      0.93         8\n",
      "        20.0       0.94      0.94      0.94        17\n",
      "        21.0       0.62      0.59      0.61        17\n",
      "        22.0       0.64      0.73      0.68        22\n",
      "        23.0       0.83      1.00      0.91        15\n",
      "        24.0       0.95      1.00      0.97        19\n",
      "        25.0       0.33      0.33      0.33         3\n",
      "        26.0       0.82      0.75      0.78        12\n",
      "        27.0       0.50      0.62      0.56         8\n",
      "        28.0       0.73      0.61      0.67        18\n",
      "        29.0       0.96      0.96      0.96        27\n",
      "        30.0       0.89      0.73      0.80        11\n",
      "        31.0       0.33      0.50      0.40         2\n",
      "        32.0       0.53      0.42      0.47        19\n",
      "        33.0       0.83      0.89      0.86        28\n",
      "        34.0       0.75      0.83      0.79        18\n",
      "        35.0       0.00      0.00      0.00         0\n",
      "        36.0       0.67      0.60      0.63        10\n",
      "        37.0       1.00      0.67      0.80         3\n",
      "        38.0       0.88      0.86      0.87        35\n",
      "        39.0       0.85      1.00      0.92        11\n",
      "        40.0       0.78      0.82      0.80        17\n",
      "        41.0       0.50      0.50      0.50         2\n",
      "        42.0       0.61      0.52      0.56        21\n",
      "        43.0       0.00      0.00      0.00         2\n",
      "        44.0       0.58      1.00      0.74         7\n",
      "        45.0       0.65      0.65      0.65        20\n",
      "        46.0       0.91      1.00      0.95        10\n",
      "        47.0       0.74      0.61      0.67        28\n",
      "        48.0       0.71      0.38      0.50        13\n",
      "        49.0       0.67      0.57      0.62         7\n",
      "        50.0       0.40      0.33      0.36         6\n",
      "        51.0       0.95      0.84      0.89        25\n",
      "        52.0       0.00      0.00      0.00         1\n",
      "        53.0       0.82      1.00      0.90         9\n",
      "        54.0       0.42      0.62      0.50        13\n",
      "        55.0       0.78      0.88      0.82         8\n",
      "        56.0       0.50      0.40      0.44         5\n",
      "        57.0       0.38      1.00      0.56         5\n",
      "        58.0       0.86      0.67      0.75         9\n",
      "        59.0       1.00      0.80      0.89         5\n",
      "        60.0       0.85      0.92      0.88        12\n",
      "        61.0       0.86      1.00      0.92         6\n",
      "        62.0       0.62      0.45      0.53        11\n",
      "        63.0       0.25      1.00      0.40         1\n",
      "        64.0       0.86      0.86      0.86        29\n",
      "        65.0       0.73      0.84      0.78        43\n",
      "        66.0       0.50      1.00      0.67         1\n",
      "        67.0       0.69      0.60      0.64        15\n",
      "        68.0       0.67      0.46      0.55        13\n",
      "        69.0       0.69      0.90      0.78        10\n",
      "        70.0       0.62      0.70      0.65        23\n",
      "        71.0       0.70      0.64      0.67        11\n",
      "        72.0       0.62      0.43      0.51        23\n",
      "        73.0       0.75      0.75      0.75        20\n",
      "        74.0       0.82      0.82      0.82        17\n",
      "        75.0       0.86      0.90      0.88        20\n",
      "        76.0       0.82      0.82      0.82        38\n",
      "        77.0       0.67      1.00      0.80        10\n",
      "        78.0       0.00      0.00      0.00         1\n",
      "        79.0       0.91      0.77      0.83        13\n",
      "        80.0       0.00      0.00      0.00         3\n",
      "        81.0       0.78      0.58      0.67        12\n",
      "        82.0       1.00      1.00      1.00         1\n",
      "        83.0       1.00      0.14      0.25         7\n",
      "        84.0       1.00      0.60      0.75         5\n",
      "        85.0       0.56      0.91      0.69        11\n",
      "        86.0       0.50      0.33      0.40         3\n",
      "        87.0       0.62      0.83      0.71        12\n",
      "        88.0       0.51      0.64      0.57        39\n",
      "        89.0       0.00      0.00      0.00         3\n",
      "        90.0       0.27      0.40      0.32        10\n",
      "        91.0       0.62      0.62      0.62        16\n",
      "        92.0       0.25      0.25      0.25         4\n",
      "        93.0       0.50      0.50      0.50         4\n",
      "        94.0       0.62      0.62      0.62        24\n",
      "        95.0       0.73      1.00      0.84         8\n",
      "        96.0       0.92      0.86      0.89        28\n",
      "        97.0       0.69      0.69      0.69        16\n",
      "        98.0       0.67      0.62      0.65        16\n",
      "        99.0       1.00      0.89      0.94        27\n",
      "       100.0       0.40      1.00      0.57         2\n",
      "       101.0       0.43      0.38      0.40         8\n",
      "       102.0       0.00      0.00      0.00         1\n",
      "       103.0       1.00      1.00      1.00         1\n",
      "       104.0       0.33      0.67      0.44         3\n",
      "       105.0       0.64      0.58      0.61        12\n",
      "       106.0       0.36      0.67      0.47         6\n",
      "       107.0       1.00      1.00      1.00         1\n",
      "       108.0       0.60      0.75      0.67         4\n",
      "       109.0       0.00      0.00      0.00         1\n",
      "       110.0       0.00      0.00      0.00         1\n",
      "       111.0       0.20      0.14      0.17         7\n",
      "       112.0       0.93      0.74      0.82        19\n",
      "       113.0       0.88      1.00      0.93         7\n",
      "       114.0       0.67      0.67      0.67         3\n",
      "       115.0       0.50      1.00      0.67         1\n",
      "       116.0       0.00      0.00      0.00         1\n",
      "       117.0       0.98      0.90      0.94        58\n",
      "       118.0       0.62      1.00      0.77         5\n",
      "       119.0       0.50      0.43      0.46         7\n",
      "       120.0       0.73      0.56      0.63        34\n",
      "       121.0       0.92      0.71      0.80        17\n",
      "\n",
      "    accuracy                           0.72      1549\n",
      "   macro avg       0.62      0.63      0.61      1549\n",
      "weighted avg       0.73      0.72      0.72      1549\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test_tree, tree_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 618
    },
    "colab_type": "code",
    "id": "ejJHglCBTV77",
    "outputId": "d3367afe-ad24-42e6-d464-8e25e93333a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 160 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:667: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:  5.9min\n",
      "[Parallel(n_jobs=-1)]: Done 124 tasks      | elapsed: 25.9min\n",
      "[Parallel(n_jobs=-1)]: Done 284 tasks      | elapsed: 58.2min\n",
      "[Parallel(n_jobs=-1)]: Done 480 out of 480 | elapsed: 96.5min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score=nan,\n",
       "             estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
       "                                              class_weight=None,\n",
       "                                              criterion='gini', max_depth=None,\n",
       "                                              max_features='auto',\n",
       "                                              max_leaf_nodes=None,\n",
       "                                              max_samples=None,\n",
       "                                              min_impurity_decrease=0.0,\n",
       "                                              min_impurity_split=None,\n",
       "                                              min_samples_leaf=1,\n",
       "                                              min_samples_split=2,\n",
       "                                              min_weight_fraction_leaf=0.0,\n",
       "                                              n_estimators=100, n_jobs=None,\n",
       "                                              oob_score=False,\n",
       "                                              random_state=None, verbose=0,\n",
       "                                              warm_start=False),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid={'criterion': ['entropy'],\n",
       "                         'min_samples_leaf': [1, 2, 3, 4, 5],\n",
       "                         'min_samples_split': [2, 3, 4, 5, 6, 7, 8, 9],\n",
       "                         'n_estimators': [90, 100, 110, 120], 'n_jobs': [-1],\n",
       "                         'random_state': [23]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=3)"
      ]
     },
     "execution_count": 56,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grid search for random forest\n",
    "params = {'criterion':[\"entropy\"],\n",
    " 'n_estimators':[90,100,110,120], # how many trees to consider\n",
    " 'min_samples_split':[2, 3, 4, 5, 6, 7, 8, 9], # every indiv tree: controls overfitting, also depth of tree\n",
    " 'min_samples_leaf':[1, 2, 3, 4, 5], # every indiv tree: controls overfitting, leaves need not have zero entropy\n",
    " 'random_state':[23],\n",
    " 'n_jobs':[-1]}\n",
    "\n",
    "X_train_rf, y_train_rf, X_test_rf, y_test_rf = util_vanilla(df_rf)\n",
    "\n",
    "grid_rf = GridSearchCV(RandomForestClassifier(), param_grid=params, cv=3, n_jobs=-1, verbose=3)\n",
    "grid_rf.fit(X_train_rf, y_train_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 308
    },
    "colab_type": "code",
    "id": "M74yN-u5qRxW",
    "outputId": "b2220eb4-7f9d-4994-a7cc-e56c60e658c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best hyperparams:  {'criterion': 'entropy', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 110, 'n_jobs': -1, 'random_state': 23}\n",
      "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='entropy', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=110,\n",
      "                       n_jobs=-1, oob_score=False, random_state=23, verbose=0,\n",
      "                       warm_start=False)\n",
      "\n",
      "\n",
      "Train set\n",
      "Accuracy: 1.0\n",
      ".......................\n",
      "Test set\n",
      "Accuracy: 0.9373789541639768\n"
     ]
    }
   ],
   "source": [
    "print(\"best hyperparams: \", grid_rf.best_params_)\n",
    "print(grid_rf.best_estimator_)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Train set\")\n",
    "rf_pred_train = grid_rf.predict(X_train_rf)\n",
    "print(\"Accuracy:\",accuracy_score(y_train_rf, rf_pred_train))\n",
    "\n",
    "print(\".......................\")\n",
    "\n",
    "print(\"Test set\")\n",
    "rf_pred_test = grid_rf.predict(X_test_rf)\n",
    "print(\"Accuracy:\",accuracy_score(y_test_rf, rf_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "duHTQr_4rBEe",
    "outputId": "e2722889-003e-4290-eec2-d11257c3e5e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00        39\n",
      "         1.0       1.00      1.00      1.00        48\n",
      "         2.0       1.00      1.00      1.00        30\n",
      "         3.0       1.00      1.00      1.00        23\n",
      "         4.0       1.00      1.00      1.00        21\n",
      "         5.0       1.00      1.00      1.00        10\n",
      "         6.0       1.00      1.00      1.00         8\n",
      "         7.0       1.00      1.00      1.00        15\n",
      "         8.0       1.00      1.00      1.00        36\n",
      "         9.0       1.00      1.00      1.00        32\n",
      "        10.0       1.00      1.00      1.00        16\n",
      "        11.0       1.00      1.00      1.00        14\n",
      "        12.0       1.00      1.00      1.00        46\n",
      "        13.0       1.00      1.00      1.00        50\n",
      "        14.0       1.00      1.00      1.00         3\n",
      "        15.0       1.00      1.00      1.00        11\n",
      "        16.0       1.00      1.00      1.00        41\n",
      "        17.0       1.00      1.00      1.00        53\n",
      "        18.0       1.00      1.00      1.00       132\n",
      "        19.0       1.00      1.00      1.00        18\n",
      "        20.0       1.00      1.00      1.00        40\n",
      "        21.0       1.00      1.00      1.00        41\n",
      "        22.0       1.00      1.00      1.00        52\n",
      "        23.0       1.00      1.00      1.00        34\n",
      "        24.0       1.00      1.00      1.00        43\n",
      "        25.0       1.00      1.00      1.00         7\n",
      "        26.0       1.00      1.00      1.00        27\n",
      "        27.0       1.00      1.00      1.00        18\n",
      "        28.0       1.00      1.00      1.00        42\n",
      "        29.0       1.00      1.00      1.00        62\n",
      "        30.0       1.00      1.00      1.00        27\n",
      "        31.0       1.00      1.00      1.00         5\n",
      "        32.0       1.00      1.00      1.00        43\n",
      "        33.0       1.00      1.00      1.00        66\n",
      "        34.0       1.00      1.00      1.00        41\n",
      "        35.0       1.00      1.00      1.00         1\n",
      "        36.0       1.00      1.00      1.00        25\n",
      "        37.0       1.00      1.00      1.00         7\n",
      "        38.0       1.00      1.00      1.00        80\n",
      "        39.0       1.00      1.00      1.00        26\n",
      "        40.0       1.00      1.00      1.00        41\n",
      "        41.0       1.00      1.00      1.00         5\n",
      "        42.0       1.00      1.00      1.00        50\n",
      "        43.0       1.00      1.00      1.00         4\n",
      "        44.0       1.00      1.00      1.00        15\n",
      "        45.0       1.00      1.00      1.00        48\n",
      "        46.0       1.00      1.00      1.00        22\n",
      "        47.0       1.00      1.00      1.00        64\n",
      "        48.0       1.00      1.00      1.00        30\n",
      "        49.0       1.00      1.00      1.00        15\n",
      "        50.0       1.00      1.00      1.00        14\n",
      "        51.0       1.00      1.00      1.00        57\n",
      "        52.0       1.00      1.00      1.00         3\n",
      "        53.0       1.00      1.00      1.00        21\n",
      "        54.0       1.00      1.00      1.00        29\n",
      "        55.0       1.00      1.00      1.00        18\n",
      "        56.0       1.00      1.00      1.00        11\n",
      "        57.0       1.00      1.00      1.00        12\n",
      "        58.0       1.00      1.00      1.00        22\n",
      "        59.0       1.00      1.00      1.00        12\n",
      "        60.0       1.00      1.00      1.00        29\n",
      "        61.0       1.00      1.00      1.00        14\n",
      "        62.0       1.00      1.00      1.00        25\n",
      "        63.0       1.00      1.00      1.00         1\n",
      "        64.0       1.00      1.00      1.00        69\n",
      "        65.0       1.00      1.00      1.00        99\n",
      "        66.0       1.00      1.00      1.00         3\n",
      "        67.0       1.00      1.00      1.00        35\n",
      "        68.0       1.00      1.00      1.00        32\n",
      "        69.0       1.00      1.00      1.00        25\n",
      "        70.0       1.00      1.00      1.00        54\n",
      "        71.0       1.00      1.00      1.00        25\n",
      "        72.0       1.00      1.00      1.00        54\n",
      "        73.0       1.00      1.00      1.00        47\n",
      "        74.0       1.00      1.00      1.00        41\n",
      "        75.0       1.00      1.00      1.00        46\n",
      "        76.0       1.00      1.00      1.00        90\n",
      "        77.0       1.00      1.00      1.00        25\n",
      "        78.0       1.00      1.00      1.00         1\n",
      "        79.0       1.00      1.00      1.00        29\n",
      "        80.0       1.00      1.00      1.00         7\n",
      "        81.0       1.00      1.00      1.00        27\n",
      "        82.0       1.00      1.00      1.00         3\n",
      "        83.0       1.00      1.00      1.00        17\n",
      "        84.0       1.00      1.00      1.00        11\n",
      "        85.0       1.00      1.00      1.00        27\n",
      "        86.0       1.00      1.00      1.00         7\n",
      "        87.0       1.00      1.00      1.00        28\n",
      "        88.0       1.00      1.00      1.00        92\n",
      "        89.0       1.00      1.00      1.00         7\n",
      "        90.0       1.00      1.00      1.00        23\n",
      "        91.0       1.00      1.00      1.00        38\n",
      "        92.0       1.00      1.00      1.00         8\n",
      "        93.0       1.00      1.00      1.00         8\n",
      "        94.0       1.00      1.00      1.00        56\n",
      "        95.0       1.00      1.00      1.00        19\n",
      "        96.0       1.00      1.00      1.00        65\n",
      "        97.0       1.00      1.00      1.00        38\n",
      "        98.0       1.00      1.00      1.00        37\n",
      "        99.0       1.00      1.00      1.00        64\n",
      "       100.0       1.00      1.00      1.00         5\n",
      "       101.0       1.00      1.00      1.00        18\n",
      "       102.0       1.00      1.00      1.00         1\n",
      "       103.0       1.00      1.00      1.00         1\n",
      "       104.0       1.00      1.00      1.00         6\n",
      "       105.0       1.00      1.00      1.00        28\n",
      "       106.0       1.00      1.00      1.00        14\n",
      "       107.0       1.00      1.00      1.00         3\n",
      "       108.0       1.00      1.00      1.00        11\n",
      "       109.0       1.00      1.00      1.00         1\n",
      "       110.0       1.00      1.00      1.00         3\n",
      "       111.0       1.00      1.00      1.00        15\n",
      "       112.0       1.00      1.00      1.00        46\n",
      "       113.0       1.00      1.00      1.00        16\n",
      "       114.0       1.00      1.00      1.00         8\n",
      "       115.0       1.00      1.00      1.00         1\n",
      "       116.0       1.00      1.00      1.00         2\n",
      "       117.0       1.00      1.00      1.00       136\n",
      "       118.0       1.00      1.00      1.00        12\n",
      "       119.0       1.00      1.00      1.00        16\n",
      "       120.0       1.00      1.00      1.00        79\n",
      "       121.0       1.00      1.00      1.00        40\n",
      "\n",
      "    accuracy                           1.00      3614\n",
      "   macro avg       1.00      1.00      1.00      3614\n",
      "weighted avg       1.00      1.00      1.00      3614\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_train_rf, rf_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "B8JtNSoRrDTG",
    "outputId": "55c2f9fe-157e-4a20-b64c-55bc188bad1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      1.00      0.89        17\n",
      "           1       0.91      0.95      0.93        21\n",
      "           2       0.93      1.00      0.96        13\n",
      "           3       1.00      0.90      0.95        10\n",
      "           4       1.00      0.89      0.94         9\n",
      "           5       1.00      0.50      0.67         4\n",
      "           6       1.00      1.00      1.00         4\n",
      "           7       0.75      0.86      0.80         7\n",
      "           8       1.00      0.93      0.97        15\n",
      "           9       0.93      1.00      0.96        13\n",
      "          10       1.00      0.57      0.73         7\n",
      "          11       1.00      1.00      1.00         6\n",
      "          12       1.00      1.00      1.00        19\n",
      "          13       0.91      0.95      0.93        22\n",
      "          14       0.00      0.00      0.00         1\n",
      "          15       0.83      1.00      0.91         5\n",
      "          16       1.00      0.94      0.97        17\n",
      "          17       0.88      1.00      0.94        22\n",
      "          18       1.00      1.00      1.00        56\n",
      "          19       1.00      1.00      1.00         8\n",
      "          20       0.89      1.00      0.94        17\n",
      "          21       1.00      1.00      1.00        17\n",
      "          22       1.00      0.95      0.98        22\n",
      "          23       0.94      1.00      0.97        15\n",
      "          24       1.00      1.00      1.00        19\n",
      "          25       1.00      0.67      0.80         3\n",
      "          26       1.00      1.00      1.00        12\n",
      "          27       0.89      1.00      0.94         8\n",
      "          28       0.80      0.89      0.84        18\n",
      "          29       0.96      1.00      0.98        27\n",
      "          30       1.00      0.73      0.84        11\n",
      "          31       1.00      1.00      1.00         2\n",
      "          32       0.81      0.89      0.85        19\n",
      "          33       1.00      0.96      0.98        28\n",
      "          34       1.00      0.94      0.97        18\n",
      "          36       0.91      1.00      0.95        10\n",
      "          37       1.00      1.00      1.00         3\n",
      "          38       1.00      1.00      1.00        35\n",
      "          39       1.00      1.00      1.00        11\n",
      "          40       0.89      1.00      0.94        17\n",
      "          41       1.00      1.00      1.00         2\n",
      "          42       0.84      1.00      0.91        21\n",
      "          43       1.00      0.50      0.67         2\n",
      "          44       0.88      1.00      0.93         7\n",
      "          45       0.95      0.95      0.95        20\n",
      "          46       1.00      1.00      1.00        10\n",
      "          47       0.96      0.93      0.95        28\n",
      "          48       0.86      0.92      0.89        13\n",
      "          49       1.00      0.86      0.92         7\n",
      "          50       1.00      0.50      0.67         6\n",
      "          51       1.00      1.00      1.00        25\n",
      "          52       1.00      1.00      1.00         1\n",
      "          53       1.00      1.00      1.00         9\n",
      "          54       0.83      0.77      0.80        13\n",
      "          55       1.00      0.75      0.86         8\n",
      "          56       0.75      0.60      0.67         5\n",
      "          57       1.00      1.00      1.00         5\n",
      "          58       0.89      0.89      0.89         9\n",
      "          59       1.00      1.00      1.00         5\n",
      "          60       1.00      1.00      1.00        12\n",
      "          61       1.00      1.00      1.00         6\n",
      "          62       0.83      0.91      0.87        11\n",
      "          63       0.00      0.00      0.00         1\n",
      "          64       0.96      0.93      0.95        29\n",
      "          65       0.89      0.98      0.93        43\n",
      "          66       1.00      1.00      1.00         1\n",
      "          67       0.85      0.73      0.79        15\n",
      "          68       0.91      0.77      0.83        13\n",
      "          69       1.00      0.90      0.95        10\n",
      "          70       0.91      0.87      0.89        23\n",
      "          71       0.91      0.91      0.91        11\n",
      "          72       0.88      0.96      0.92        23\n",
      "          73       0.95      0.90      0.92        20\n",
      "          74       1.00      1.00      1.00        17\n",
      "          75       0.95      0.95      0.95        20\n",
      "          76       0.90      0.97      0.94        38\n",
      "          77       1.00      1.00      1.00        10\n",
      "          78       0.00      0.00      0.00         1\n",
      "          79       1.00      1.00      1.00        13\n",
      "          80       1.00      1.00      1.00         3\n",
      "          81       1.00      0.83      0.91        12\n",
      "          82       1.00      1.00      1.00         1\n",
      "          83       1.00      0.86      0.92         7\n",
      "          84       1.00      1.00      1.00         5\n",
      "          85       0.91      0.91      0.91        11\n",
      "          86       1.00      1.00      1.00         3\n",
      "          87       1.00      0.92      0.96        12\n",
      "          88       0.88      0.90      0.89        39\n",
      "          89       1.00      0.67      0.80         3\n",
      "          90       1.00      0.90      0.95        10\n",
      "          91       0.84      1.00      0.91        16\n",
      "          92       1.00      1.00      1.00         4\n",
      "          93       1.00      0.75      0.86         4\n",
      "          94       1.00      0.96      0.98        24\n",
      "          95       1.00      1.00      1.00         8\n",
      "          96       0.93      1.00      0.97        28\n",
      "          97       1.00      1.00      1.00        16\n",
      "          98       0.83      0.94      0.88        16\n",
      "          99       1.00      1.00      1.00        27\n",
      "         100       1.00      1.00      1.00         2\n",
      "         101       1.00      1.00      1.00         8\n",
      "         102       0.00      0.00      0.00         1\n",
      "         103       0.00      0.00      0.00         1\n",
      "         104       1.00      0.33      0.50         3\n",
      "         105       1.00      0.67      0.80        12\n",
      "         106       0.86      1.00      0.92         6\n",
      "         107       1.00      1.00      1.00         1\n",
      "         108       1.00      1.00      1.00         4\n",
      "         109       0.00      0.00      0.00         1\n",
      "         110       1.00      1.00      1.00         1\n",
      "         111       1.00      1.00      1.00         7\n",
      "         112       0.89      0.89      0.89        19\n",
      "         113       1.00      1.00      1.00         7\n",
      "         114       1.00      1.00      1.00         3\n",
      "         115       1.00      1.00      1.00         1\n",
      "         116       1.00      1.00      1.00         1\n",
      "         117       1.00      0.91      0.95        58\n",
      "         118       1.00      1.00      1.00         5\n",
      "         119       0.78      1.00      0.88         7\n",
      "         120       0.78      0.94      0.85        34\n",
      "         121       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.94      1549\n",
      "   macro avg       0.91      0.88      0.89      1549\n",
      "weighted avg       0.94      0.94      0.93      1549\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test_rf, rf_pred_test))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "leaves_dt_rf.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
