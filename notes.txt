features: (dimensions)
..smoothness: 1
..leaflet_desc: 7
..gabor: 8
..color: 12
..fft: 50
..geom: 51

target:
..lobe_ct: values = 1, 2, or 3

models:
..decision trees - tuning: pruning	
..logistic regression
..svm
..neural network

preprocessing
..do we need to one-hot encode?? --> not needed
..bagging/boosting... xgboost??
..pearson's correlation
..PCA

decision tree
..out of the box random splitting gives us ~0.65 accuracy
..criterion for splitting can be gini impurity or entropy - entropy better for
high imbalance in classes, else both perform very similarly (entropy is computationally
more intensive) 

points to note:
..try stratification/any other methods to deal with class imbalance
..deciding the features
	color
	fft
..grid search
..Standard classifier algorithms like Decision Tree and Logistic Regression have a bias towards 
classes which have large number of instances. They tend to only predict the majority class data. The 
features of the minority class are treated as noise and are often ignored. Thus, there is a high 
probability of misclassification of the minority class as compared to the majority class.
..while working in an imbalanced domain accuracy is not an appropriate measure to evaluate model 
performance. For eg: A classifier which achieves an accuracy of 98 % with an event rate of 2 % 
is not accurate, if it classifies all instances as the majority class. And eliminates the 2 % 
minority class observations as noise.

NN:
preprocessing:
pref bw 0,1
normalize (or) standardize first and then normalize

SVM
preprocessing:
soft standardization (sklearn standardscaler)
hyperparameters:
unbalanced input data - inverse training weights??
.. classification error - high: focus more on classification, smaller margin, low: focus is more on getting a wider margin
(tuning of hyperparameters using grid search)
.. degree of polynomial
.. gamma for radial

when to use normalisation and standardisation?
normalisation: for algorithms that require the parameters/attributes to be in [0,1]
e.g. ANNs, CNNs, logistic regression, simple linear regression
so that the minima can be attained without too great heights (due to huge differences
in the ranges in the features)
standardisation: for algorithms that are more distance based, like SVM, KNN, etc.
